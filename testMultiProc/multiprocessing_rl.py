# -*- coding: utf-8 -*-
"""multiprocessing_rl.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/multiprocessing_rl.ipynb

# Stable Baselines3 - Easy Multiprocessing

Github Repo: [https://github.com/DLR-RM/stable-baselines3](https://github.com/DLR-RM/stable-baselines3)


[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a training framework for Reinforcement Learning (RL), using Stable Baselines3.

It provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.

Documentation is available online: [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/)

## Install Dependencies and Stable Baselines Using Pip


```
pip install stable-baselines3[extra]
"""

# for autoformatting
# %load_ext jupyter_black

# !pip install "stable-baselines3[extra]>=2.0.0a4"

"""## Import policy, RL agent, ..."""

import time

import gymnasium as gym
import numpy as np

from stable_baselines3 import A2C
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from stable_baselines3.common.utils import set_random_seed
from stable_baselines3.common.evaluation import evaluate_policy
from stable_baselines3.common.env_util import make_vec_env
from envMultiProc import envMultiProc


"""## Multiprocessing RL Training

To multiprocess RL training, we will just have to wrap the Gym env into a `SubprocVecEnv` object, that will take care of synchronising the processes. The idea is that each process will run an indepedent instance of the Gym env.

For that, we need an additional utility function, `make_env`, that will instantiate the environments and make sure they are different (using different random seed).
"""

from typing import Callable

def main():
    def make_env(env_id: str, rank: int, seed: int = 0) -> Callable:
        """
        Utility function for multiprocessed env.

        :param env_id: (str) the environment ID
        :param num_env: (int) the number of environment you wish to have in subprocesses
        :param seed: (int) the inital seed for RNG
        :param rank: (int) index of the subprocess
        :return: (Callable)
        """

        def _init() -> gym.Env:
            env = envMultiProc()
            env.reset(seed=seed + rank)
            return env

        set_random_seed(seed)
        return _init

    """The number of parallel process used is defined by the `num_cpu` variable.

    Because we use vectorized environment (SubprocVecEnv), the actions sent to the wrapped env must be an array (one action per process). Also, observations, rewards and dones are arrays.
    """

    env_id = "CartPole-v1"
    num_cpu = 10  # Number of processes to use
    # Create the vectorized environment
    env_dummy = DummyVecEnv([make_env(env_id, i) for i in range(num_cpu)])
    # env_subprocVec = SubprocVecEnv([make_env(env_id, i) for i in range(num_cpu)])
    env_makeVec = make_vec_env(env_id, n_envs=num_cpu)

    model = A2C("MlpPolicy", env_id, verbose=0)
    model_dummy = A2C("MlpPolicy", env_dummy, verbose=0)
    # model_subprocVec = A2C("MlpPolicy", vec_subprocVec, verbose=0)
    model_makeVec = A2C("MlpPolicy", env_makeVec, verbose=0)

    n_timesteps = 25_000

    # Single Process RL Training
    start_time = time.time()
    model.learn(n_timesteps)
    total_time_multi = time.time() - start_time
    print(f"Took {total_time_multi:.2f}s for single process version - {n_timesteps / total_time_multi:.2f} FPS")

    # Dummy RL Training
    start_time = time.time()
    model_dummy.learn(n_timesteps)
    total_time_multi = time.time() - start_time
    print(f"Took {total_time_multi:.2f}s for dummy version - {n_timesteps / total_time_multi:.2f} FPS")
    
    # subprocVec RL Training
    start_time = time.time()
    model_subprocVec.learn(n_timesteps)
    total_time_multi = time.time() - start_time
    print(f"Took {total_time_multi:.2f}s for subprocVec version - {n_timesteps / total_time_multi:.2f} FPS")

    # makeVec RL Training
    start_time = time.time()
    model_makeVec.learn(n_timesteps)
    total_time_multi = time.time() - start_time
    print(f"Took {total_time_multi:.2f}s for makeVec version - {n_timesteps / total_time_multi:.2f} FPS")
 
    # evaluate_policy(model, gym.make(env_id), n_eval_episodes=10, render=False)


if __name__ == '__main__':
    main()